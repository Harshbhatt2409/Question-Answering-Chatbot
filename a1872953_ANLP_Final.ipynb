{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "j2Zib-ZH_OAU"
      ],
      "authorship_tag": "ABX9TyNhHHx5RINlMxRAvtFRiV81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshbhatt2409/Question-Answering-Chatbot/blob/main/a1872953_ANLP_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**a1872953**\n",
        "\n",
        "**Harsh Alpesh Bhatt**\n",
        "\n",
        "**Applied Natural Language Processing Final Assignment**\n",
        "\n",
        "**Task: Develop a question-answering system similar to ChatPDF that are focused on the research papers in AI**"
      ],
      "metadata": {
        "id": "nfEdxhjdNuz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Installation of Libraries"
      ],
      "metadata": {
        "id": "j2Zib-ZH_OAU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "zbDhp8qG1FRD",
        "outputId": "5bf07f2c-bf03-40c3-8c55-a4da1b39025b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.9.0-py3-none-any.whl (46 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow>=9.1 (from pdfplumber)\n",
            "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Wand>=0.6.10 (from pdfplumber)\n",
            "  Downloading Wand-0.6.11-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.6/143.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (2.0.12)\n",
            "Collecting cryptography>=36.0.0 (from pdfminer.six==20221105->pdfplumber)\n",
            "  Downloading cryptography-41.0.1-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: Wand, Pillow, cryptography, pdfminer.six, pdfplumber\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "Successfully installed Pillow-9.5.0 Wand-0.6.11 cryptography-41.0.1 pdfminer.six-20221105 pdfplumber-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q-dB5j91eqa",
        "outputId": "710eebef-3c4e-4551-d6a8-65ddbd8ebca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Collecting openai\n",
            "  Using cached openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Importing Libraries"
      ],
      "metadata": {
        "id": "2e_xMgKw_S3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from nltk import ne_chunk"
      ],
      "metadata": {
        "id": "9LbYx3TyLO6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "CAgzvCF1LQ1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2b3xSySLTCy",
        "outputId": "0586fe96-8112-4774-d8df-28efd50961ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Open AI Key"
      ],
      "metadata": {
        "id": "c1EYKcZc_Ygn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up OpenAI API credentials\n",
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "z1HuXUFjLJBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Question Answering System"
      ],
      "metadata": {
        "id": "k0kaVyYE_fDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a function to load the PDF file"
      ],
      "metadata": {
        "id": "d7PD3h-z_j2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this function we will also perforn the text tokenisation, lowercasing the sentences, Lemmatization and removal of stopwords."
      ],
      "metadata": {
        "id": "iYZe4wRs_oAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_paper(file_path):\n",
        "    # Load the PDF file and extract the text\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "    # Preprocess the text\n",
        "    text = text.replace('\\n', ' ')\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    processed_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        filtered_sentence = [lemmatizer.lemmatize(word) for word in sentence if word.isalnum() and word not in stop_words]\n",
        "        processed_sentences.append(filtered_sentence)\n",
        "\n",
        "    return processed_sentences"
      ],
      "metadata": {
        "id": "8p3o7F7YLyyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now perforn the step of Named Entity Recognition(NER) and define a function for the same"
      ],
      "metadata": {
        "id": "3GHveLrO_zyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entity recognition (NER) is a natural language processing (NLP) method that extracts information from text. NER involves detecting and categorizing important information in text known as named entities."
      ],
      "metadata": {
        "id": "dpnAqdNhAJ6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_named_entities(sentence):\n",
        "    # Extract named entities from a sentence using NER\n",
        "    chunked = ne_chunk(nltk.pos_tag(sentence))\n",
        "    named_entities = []\n",
        "    for chunk in chunked:\n",
        "        if hasattr(chunk, 'label') and chunk.label() == 'NE':\n",
        "            named_entities.append(' '.join(child[0] for child in chunk))\n",
        "\n",
        "    return named_entities"
      ],
      "metadata": {
        "id": "PTN2vL9aMC1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define a function to calcualte the similarity between the sentence and the question"
      ],
      "metadata": {
        "id": "0KEtoXqgAl2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity(sentence, question):\n",
        "    # Calculate the similarity between a sentence and a question using edit distance\n",
        "    return edit_distance(sentence.lower(), question.lower())"
      ],
      "metadata": {
        "id": "ZvKNnjMhMGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a function to perform the question answering step"
      ],
      "metadata": {
        "id": "05v10xQ8AyNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used ChatGPT - 3 for our model and used an advanced ensemble method to iteratively ask the same question to the model for better results."
      ],
      "metadata": {
        "id": "UIg8JcJwBQ5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also perform a step for similairty check between the answers and the questions and the multiple answers that are generated by the model. We select the answer that is best fit for the given question"
      ],
      "metadata": {
        "id": "6BYsHIutBdnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(paper_data, question):\n",
        "    # Use OpenAI ChatGPT for answering questions\n",
        "    prompt = f\"Question: {question}\\nAnswer:\"\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        max_tokens=800,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    answer = response.choices[0].text.strip()\n",
        "\n",
        "    # Find the most relevant sentence in the paper data based on similarity and named entities\n",
        "    max_similarity = float('inf')\n",
        "    relevant_sentence = \"\"\n",
        "    for sentence in paper_data:\n",
        "        similarity = calculate_similarity(' '.join(sentence), question)\n",
        "        entities = extract_named_entities(sentence)\n",
        "        if similarity < max_similarity and entities:\n",
        "            max_similarity = similarity\n",
        "            relevant_sentence = ' '.join(sentence)\n",
        "\n",
        "    # Concatenate the sentences of the answer into a single paragraph\n",
        "    paragraph = relevant_sentence + ' ' + answer\n",
        "\n",
        "    return paragraph"
      ],
      "metadata": {
        "id": "Gwp09psPMi0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the ensemble function i have used to repeatatively ask the model for the answer for the same question."
      ],
      "metadata": {
        "id": "Qza3AWRwBvYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It calculates the similarity of the answer generated by the model and the question and keeps the most similar answer"
      ],
      "metadata": {
        "id": "tf_SNxAQB7c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_answer(paper_data, question):\n",
        "    # Use an ensemble of models to generate the answer\n",
        "    answers = []\n",
        "    for _ in range(100):\n",
        "        answer = answer_question(paper_data, question)\n",
        "        answers.append(answer)\n",
        "\n",
        "    # Calculate similarities and choose the answer with the highest similarity\n",
        "    max_similarity = float('-inf')\n",
        "    best_answer = \"\"\n",
        "    for answer in answers:\n",
        "        similarity = calculate_similarity(answer, question)\n",
        "        if similarity > max_similarity:\n",
        "            max_similarity = similarity\n",
        "            best_answer = answer\n",
        "\n",
        "    return best_answer"
      ],
      "metadata": {
        "id": "0Ae5bnoWM4y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also have kept a function to store the answers and also provide the model the correct answer for it to train and improve the accuracy of the overall model."
      ],
      "metadata": {
        "id": "rfBtMBmiCE-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_correct_answer(question, correct_answer):\n",
        "    # Store the correct answer for a given question\n",
        "    if question in correct_answers:\n",
        "        correct_answers[question].append(correct_answer)\n",
        "    else:\n",
        "        correct_answers[question] = [correct_answer]\n",
        "\n",
        "def get_correct_answer(question):\n",
        "    # Retrieve the correct answer for a given question\n",
        "    if question in correct_answers:\n",
        "        return correct_answers[question]\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "0E6YwKWVM6TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function"
      ],
      "metadata": {
        "id": "M7MTIu42CRgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Chat Paper style Question Answering.  We take URL as input and then the model asks for the question. It then asnwers the question and also askes if it is correct, if it is then we move onto next question or exit"
      ],
      "metadata": {
        "id": "zG85wG09CT_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the answer is not correct then we, the model asks for the correct answer and trains itself on the answer and stores the answer."
      ],
      "metadata": {
        "id": "gBC1zJ0QCouJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    file_path = input(\"Enter the URL of the PDF file: \")\n",
        "    response = requests.get(file_path)\n",
        "    pdf_data = BytesIO(response.content)\n",
        "    paper_data = load_paper(pdf_data)\n",
        "\n",
        "    # Prompt the user to ask questions\n",
        "    while True:\n",
        "        question = input(\"Ask a question or type 'exit' to quit: \")\n",
        "\n",
        "        if question.lower() == \"exit\":\n",
        "            break\n",
        "        stored_answer = get_correct_answer(question)\n",
        "        if stored_answer:\n",
        "            print(\"Answer:\")\n",
        "            print(stored_answer)\n",
        "        else:\n",
        "            answer = ensemble_answer(paper_data, question)\n",
        "            print(\"Answer:\")\n",
        "            print(answer)\n",
        "\n",
        "            # Validation step\n",
        "            user_feedback = input(\"Was the answer correct? (yes/no): \")\n",
        "            if user_feedback.lower() == \"no\":\n",
        "                correct_answer = input(\"Please provide the correct answer: \")\n",
        "                # Store the correct answer for future reference\n",
        "                store_correct_answer(question, correct_answer)\n",
        "# Dictionary to store correct answers\n",
        "correct_answers = {}\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV8r48Rw7vFX",
        "outputId": "6ec21c64-93d9-4811-ca82-dad54927b6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the URL of the PDF file: https://arxiv.org/pdf/2305.13048.pdf\n",
            "Ask a question or type 'exit' to quit:  find the key reference for the following paper\n",
            "Answer:\n",
            " The key reference for a paper is typically found in the References section at the end of the paper.  This section will typically contain the full citation for the key reference, including the author, title, journal or book, and any other relevant information.\n",
            "Was the answer correct? (yes/no): exit\n",
            "Ask a question or type 'exit' to quit: exit\n"
          ]
        }
      ]
    }
  ]
}